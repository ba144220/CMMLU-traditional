,Question,A,B,C,D,Answer
0,"在二分類問題中，當測試集的正例和負例數量不均衡時，以下評價方案哪個是相對不合理的（假設precision=TP/(TP+FP),recall=TP/(TP+FN)。）",F-值:2recallprecision/(recall+precision),G-mean:sqrt(precision*recall),準確性:(TP+TN)/all,AUC:ROC曲線下面積,C
1,深度學習中遇見過擬合下列哪個處理辦法不可取 ,加dropout層,加深層數,數據增強,加正則項,B
2,假設我們有一個數據集，在一個深度爲 6 的決策樹的幫助下，它可以使用 100% 的精確度被訓練。現在考慮一下兩點，並基於這兩點選擇正確的選項。1.深度爲 4 時將有高偏差和低方差；2.深度爲 4 時將有低偏差和低方差。注意：所有其他超參數是相同的，所有其他因子不受影響。,1 和 2,只有 2,沒有一個,只有 1,D
3,下列哪些方法不可以用來對高維數據進行降維,LASSO,Bagging,主成分分析法,聚類分析,B
4,機器學習中L1正則化和L2正則化的區別是 ,使用L1可以得到稀疏、平滑的權值,使用L2可以得到稀疏、平滑的權值,使用L1可以得到稀疏的權值、使用L2可以得到平滑的權值,使用L2可以得到稀疏的權值、使用L1可以得到平滑的權值,C
5,下列關於隱馬模型和條件隨機場模型的說法中錯誤的是? ,隱馬模型和隱馬模型隱可用於命名實體識別、分詞和詞性標註的任務,隱馬模型和隱馬模型都是生成模型,隱馬模型不是概率無向圖模型,特徵的選取和優化會嚴重影響隱馬模型的結果,B
6,下面哪個/些超參數的增加可能會造成隨機森林數據過擬合,學習速率,樹的數量,樹的深度,以上都不是,C
7,下面關於迭代二叉樹3代算法中說法錯誤的是,迭代二叉樹3代算法是一個二叉樹模型,信息增益可以用熵，而不是GINI係數來計算,迭代二叉樹3代算法要求特徵必須離散化,選取信息增益最大的特徵，作爲樹的根節點,A
8,下面哪個/些選項對 K 折交叉驗證的描述是正確的,如果 K=N，那麼其稱爲留一交叉驗證，其中 N 爲驗證集中的樣本數量,更大的 K 值相比於小 K 值將對交叉驗證結構有更高的信心,以上都是,增大 K 將導致交叉驗證結果時需要更多的時間,C
9,已知有m個樣本，進行n（n<=m）次採樣。bootstrap數據是什麼意思？,無放回地從總共N個樣本中抽樣n個樣本,無放回地從總共M個特徵中抽樣m個特徵,有放回地從總共N個樣本中抽樣n個樣本,有放回地從總共M個特徵中抽樣m個特徵,C
10,如果以特徵向量的相關係數作爲模式相似性測度，則影響聚類算法結果的主要因素有,量綱,已知類別樣本質量,以上都不是,分類準則,D
11,模型訓練過程中，我們一般將數據分成 ,驗證集,測試集,訓練集,其他選項均可,D
12,你正在使用帶有 L1 正則化的 logistic 迴歸做二分類，其中 C 是正則化參數，w1 和 w2 是 x1 和 x2 的係數。當你把 C 值從 0 增加至非常大的值時，下面哪個選項是正確的,第一個 w1 成了 0，接着 w2 也成了 0,w1 和 w2 同時成了 0,第一個 w2 成了 0，接着 w1 也成了 0,即使在 C 成爲大值之後，w1 和 w2 都不能成 0,D
13,下列方法中，不可以用於特徵降維的方法包括,深度學習SparseAutoEncoder,矩陣奇異值分解SVD,線性判別分析,主成分分析,A
14,貝葉斯定理求得是什麼概率? ,先驗概率,其它選項都不是,條件概率,聯合概率,C
15,在機器學習中，解釋學習器泛化性能中經常用到偏差-方差分解，下列說法不正確的是 ,方差體現的是學習器預測的穩定性,偏差體現的是學習器預測的準確度,泛化性能是由學習算法的能力、數據的充分性以及學習任務本身的難度所共同決定的,方差指的是預測的期望值與真實值的偏差,D
16,假如你用logistic Regression 算法去預測電腦銷量，當你在新的測試集上驗證你的假設時，發現預測值有很大的偏差，並且你的假設在訓練集上表現也很差，下面那些步驟你應該避免採納,嘗試着減小正則項 λ,嘗試增加交叉特徵,增大樣本量,嘗試更小的測試集或者特徵,D
17,以P(w)表示詞條w的概率，假設已知P（南京）=0.8，P（市長）=0.6，P（江大橋）=0.4：P（南京市）=0.3，P（長江大橋）=0.5：如果假設前後兩個詞的出現是獨立的，那麼分詞結果就是,南京_市長_江大橋,南京市_長江_大橋,南京市長_江大橋,南京市_長江大橋,A
18,對數幾率迴歸（logistics regression）和一般迴歸分析有什麼區別,對數幾率迴歸是設計用來預測事件可能性的,對數幾率迴歸可以用來估計迴歸係數,以上都是,對數幾率迴歸可以用來度量模型擬合程度,C
19,下面哪些對「類型 1（Type-1）」和「類型 2（Type-2）」錯誤的描述是錯誤的,類型 1 錯誤通常在其是正確的情況下拒絕假設而出現,類型 1 通常稱之爲假正類，類型 2 通常稱之爲假負類,以上都是,類型 2 通常稱之爲假正類，類型 1 通常稱之爲假負類,D
20,影響基本K-均值算法的主要因素有,初始類中心的選取,聚類準則,樣本輸入順序,模式相似性測度,D
21,高斯混合模型(GMM)是一種什麼模型,無監督學習模型,其他選項都不是,半監督學習模型,有監督學習模型,A
22,基於語法規則的方法爲,條件隨機場,最大熵模型,句法、語義分析,最大熵隱馬爾科夫模型,B
23,"有兩個樣本點，第一個點爲正樣本,它的特徵向量是(0,-1);第二個點爲負樣本,它的特徵向量是(2,3),從這兩個樣本點組成的訓練集構建一個線性SVM分類器的分類面方程是",2x-y=0,x+2y=5,x+2y=3,2x+y=4,C
24,隱馬爾可夫模型，設其觀察值空間爲 狀態空間爲 如果用維特比算法(Viterbi algorithm)進行解碼，時間複雜度爲,O(NK),O(N^2K),以上都不是,O(NK^2),C
25,假定你使用了一個很大γ值的RBF核，這意味着：,模型不會被點到超平面的距離所影響,以上都不是,模型僅使用接近超平面的點來建模,模型將考慮使用遠離超平面的點建模,C
26,關於 ARMA (auto regressive moving average model)（自迴歸滑動平均模型）、 AR (auto regressive model)（自迴歸模型）、 MA（滑動平均模型） 模型的功率譜，下列說法正確的是,AR模型在零點接近單位圓時，AR譜是一個尖峯,MA模型是同一個全通濾波器產生的,MA模型在極點接近單位圓時，MA譜是一個深谷,RMA譜既有尖峯又有深谷,D
27,變量選擇是用來選擇最好的判別器子集， 如果要考慮模型效率，我們應該做除了下列哪項的變量選擇的考慮,交叉驗證,變量對於模型的解釋有多大作用,特徵攜帶的信息,多個變量其實有相同的用處,B
28,"下列時間序列模型中,哪一個模型可以較好地擬合波動性的分析和預測",auto regressive model AR模型,自迴歸滑動平均模型,滑動平均模型,廣義自迴歸滑動平均模型,D
29,"當我們構造線性模型時, 我們注意變量間的相關性. 在相關矩陣中搜索相關係數時, 如果我們發現3對變量的相關係數是(Var1 和Var2, Var2和Var3, Var3和Var1)是-0.98, 0.45, 1.23 . 我們可以得出什麼結論",以上都是,"因爲Var1和Var2是非常相關的, 我們可以去除其中一個",Var1和Var2是非常相關的,Var3和Var1的1.23相關係數是不可能的,A
30,機器學習中做特徵選擇時，可能用到的方法有,以上都有,卡方,信息增益,期望交叉熵,A
31,LSTM與GRU的一個主要區別在於GRU將LSTM的哪幾個gate融合了,forget gate和input gate, input gate和output gate,forget gate和output gate,output gate和reset gate,A
32,符號集 a 、 b 、 c 、 d ，它們相互獨立，相應概率爲 1/2 、 1/4 、 1/8/ 、 1/16 ，其中包含信息量最小的符號是,d,b,a,c,C
33,假設你使用 log-loss 函數作爲評估標準。下面這些選項，哪些是對作爲評估標準的 log-loss 的正確解釋,以上都是,log-loss 越低，模型越好,對一個特別的觀察而言，分類器爲正確的類別分配非常小的概率，然後對 log-loss 的相應分佈會非常大,如果一個分類器對不正確的分類很自信，log-loss 會嚴重的批評它,A
34,"已知一組數據的協方差矩陣P,下面關於主分量說法錯誤的是",主分量分析就是K-L變換,"在經主分量分解後,協方差矩陣成爲對角矩陣","主分量分析的最佳準則是對一組數據進行按一組正交基分解, 在只取相同數量分量的條件下,以均方誤差計算截尾誤差最小",主分量是通過求協方差矩陣的特徵值得到,A
35,下列關於attention機制的說法錯誤的是,attention機制會給序列中的元素分配一個權重係數,attention機制可以用於機器閱讀理解、問答對話等場景中,傳統encoder-decoder模型存在長距離依賴問題,attention機制的變體，多頭attention機制不適合並行，其每一步計算依賴於上一步的計算結果,D
36,以下哪項不是降低過擬合的方法,收集更多訓練數據,進行數據清洗，減少噪聲,增加神經網絡隱藏層節點數,簡化模型假設,C
37,"給定三個變量 X，Y，Z。(X, Y)、(Y, Z) 和 (X, Z) 的 Pearson 相關性係數分別爲 C1、C2 和 C3。現在 X 的所有值加 2（即 X+2），Y 的全部值減 2（即 Y-2），Z 保持不變。那麼運算之後的 (X, Y)、(Y, Z) 和 (X, Z) 相關性係數分別爲 D1、D2 和 D3。現在試問 D1、D2、D3 和 C1、C2、C3 之間的關係是什麼","D1 = C1, D2 < C2, D3 < C3","D1= C1, D2 < C2, D3 > C3","D1 = C1, D2 = C2, D3 = C3","D1 = C1, D2 > C2, D3 > C3",C
38,在決策樹中，用作分裂節點的information gain說法不正確的是,信息增益更加傾向於選擇有較多取值的屬性,信息增益可以使用熵得到,較小不純度的節點需要更多的信息來區分總體,以上均不是,C
39,下列關於迴歸分析中的殘差表述正確的是,殘差的平均值總小於零,殘差的平均值總大於零,殘差的平均值總爲零,殘差沒有此類規律,C
40,"我們建立一個5000個特徵, 100萬數據的機器學習模型. 我們怎麼有效地應對這樣的大數據訓練 ","我們隨機抽取一些樣本, 在這些少量樣本之上訓練",以上所有,我們可以試用在線機器學習算法,"我們應用PCA算法降維, 減少特徵數",B
41,下列哪個不屬於條件隨機場模型對於隱馬爾科夫模型和最大熵隱馬爾科夫模型模型的優勢,速度快,可容納較多上下文信息,全局最優,特徵靈活,A
42,下列哪項不是基於詞典的方法的中文分詞的基本方法,最大熵模型,最大概率法,最大匹配法,最短路徑法,A
43,假定你使用SVM學習數據X，數據X裏面有些點存在錯誤。現在如果你使用一個二次核函數，多項式階數爲2，使用鬆弛變量C作爲超參之一。 如果使用較小的C（C趨於0），則：,不確定,誤分類,正確分類,以上均不正確,B
44,以下哪種方法屬於生成模型,條件隨機場,傳統神經網絡,樸素貝葉斯,線性迴歸,C
45,在其它條件不變的前提下，以下哪種做法容易引起機器學習中的過擬合問題,增加訓練集數量,刪除稀疏的特徵,SVM算法中使用高斯核/RBF核代替,減少神經網絡隱藏層節點數,C
46,對應GradientBoosting tree算法， 以下說法正確的是,當增加最小樣本分裂個數，我們可以抵制過擬合,當我們減少訓練單個學習器的樣本個數，我們可以降低偏差,當增加最小樣本分裂個數，會導致過擬合,當我們增加訓練單個學習器的樣本個數，我們可以降低方差,A
47,假設你訓練SVM後，得到一個線性決策邊界，你認爲該模型欠擬合。在下次迭代訓練模型時，應該考慮,減少訓練數據,減少特徵,計算更多變量,增加訓練數據,C
48,語音信號由於具有什麼特性，所以我們可以將語音信號進行分窗處理? ,隨機單調性,其他選項都不是,短時平穩性,單調不變性,C
49,"對於k折交叉驗證, 以下對k的說法正確的是","選擇更大的k, 就會有更小的bias (因爲訓練集更加接近總數據集)","k越大, 不一定越好, 選擇大的k會加大評估時間","在選擇k時, 要最小化數據集之間的方差",以上所有,D
50,以下屬於歐式距離特性的有,尺度縮放不變性,旋轉不變性,不受量綱影響的特性,考慮了模式的分佈,B
51,"樸素貝葉斯是一種特殊的貝葉斯分類器,特徵變量是X,類別標籤是C,它的一個假定是",特徵變量X的各個維度是類別條件獨立隨機變量,P(X|C)是高斯分佈,以0爲均值，sqr(2)/2爲標準差的正態分佈,各類別的先驗概率P(C)是相等的,A
52,在一個n維的空間中， 最好的檢測outlier(離羣點)的方法是,作盒形圖,作散點圖,作正態分佈概率圖,馬氏距離,D
53,對於線性迴歸模型，包括附加變量在內，以下的可能正確的是 ,R-Squared 是遞減的， Adjusted R-squared 也是遞減的,R-Squared 是常量的，Adjusted R-squared是遞增的,R-Squared 和 Adjusted R-squared都是遞增的,以上都不是,D
54,數據科學家可能會同時使用多個算法（模型）進行預測， 並且最後把這些算法的結果集成起來進行最後的預測（集成學習），以下對集成學習說法正確的是,單個模型之間有高相關性,單個模型都是用的一個算法,單個模型之間有低相關性,在集成學習中使用“平均權重”而不是“投票”會比較好,C
55,"我們想在大數據集上訓練決策樹, 爲了使用較少時間, 我們可以",增加學習率 ,減少樹的數量,增加樹的深度,減少樹的深度,D
56,"我們想要減少數據集中的特徵數, 即降維. 選擇以下適合的方案",以上所有,"我們先把所有特徵都使用, 去訓練一個模型, 得到測試集上的表現. 然後我們去掉一個特徵, 再去訓練, 用交叉驗證看看測試集上的表現. 如果表現比原來還要好, 我們可以去除這個特徵",使用前向特徵選擇方法和後向特徵排除方法,"查看相關性表, 去除相關性最高的一些特徵",A
57,以下哪個激活函數不能解決梯度彌散的問題,Leaky-Relu,Elu,Sigmoid,Relu,C
58,下列哪個不屬於常用的文本分類的特徵選擇算法,主成分分析,互信息,信息增益,卡方檢驗值,A
59,Fisher線性判別函數的求解過程是將M維特徵矢量投影在（ ）中進行求解,一維空間,三維空間,M-1維空間,二維空間,A
60,下列哪一項說明了X，Y之間的較強關係,相關係數爲0.9,都不對,Beta係數爲0的空假設的p-value是0.0001,Beta係數爲0的空假設的t統計量是30,A
61,在 k-均值算法中，以下哪個選項可用於獲得全局最小？,以上所有,找到集羣的最佳數量,調整迭代的次數,嘗試爲不同的質心（centroid）初始化運行算法,A
62,在統計語言模型中，通常以概率的形式描述任意語句的可能性，利用最大相似度估計進行度量，對於一些低頻詞，無論如何擴大訓練數據，出現的頻度仍然很低，下列哪種方法能解決這一問題,數據平滑,N元文法,一元文法,一元切分,A
63,以下說法中錯誤的是,給定n個數據點，如果其中一半用於訓練，一半用戶測試，則訓練誤差和測試誤差之間的差別會隨着n的增加而減少的,boosting和bagging都是組合多個分類器投票的方法，二者都是根據單個分類器的正確率確定其權重,SVM對噪聲（如來自其他分部的噪聲樣本）具備魯棒性,在adaboost算法中，所有被分錯樣本的權重更新比例不相同,B
64,"一個二進制源X發出符號集爲{-1,1}，經過離散無記憶信道傳輸，由於信道中噪音的存在，接收端Y收到符號集爲{-1,1,0}。已知P(x=-1)=1/4，P(x=1)=3/4，P(y=-1|x=-1)=4/5，P(y=0|x=-1)=1/5，P(y=1|x=1)=3/4，P(y=0|x=1)=1/4，求條件熵H(Y|X)",0.5372,0.2375,0.5273,0.3275,B
65,以下哪種技術對於減少數據集的維度會更好？,刪除數據差異較大的列,刪除缺少值太多的列,刪除不同數據趨勢的列,都不是,B
66,下列哪些不特別適合用來對高維數據進行降維,聚類分析,LASSO,小波分析法,拉普拉斯特徵映射,A
67,邏輯迴歸與多元迴歸分析有哪些不同？,邏輯迴歸迴歸係數的評估,邏輯迴歸預測某事件發生的概率,邏輯迴歸有較高的擬合效果,以上全選,D
68,最出名的降維算法是 PCA 和 t-SNE。將這兩個算法分別應用到數據「X」上，並得到數據集「X_projected_PCA」，「X_projected_tSNE」。下面哪一項對「X_projected_PCA」和「X_projected_tSNE」的描述是正確的,兩個都在最近鄰空間能得到解釋,X_projected_PCA 在最近鄰空間能得到解釋,兩個都不能在最近鄰空間得到解釋,X_projected_tSNE 在最近鄰空間能得到解釋,D
69,下列關於維特比算法(Viterbi)的說法中錯誤的是,維特比算法中的轉移概率是從一個隱含狀態轉移到另一個隱含狀態的概率,維特比算法是一種貪心算法,維特比算法可應用於中文分詞任務,維特比算法可得到全局最優解,B
70,以下( )不屬於線性分類器最佳準則,貝葉斯分類,感知準則函數,支持向量機,Fisher準則,A
71,對於線性迴歸，我們應該有以下哪些假設,"找到離羣點很重要, 因爲線性迴歸對離羣點很敏感",線性迴歸假設數據沒有多重線性相關性,線性迴歸要求所有變量必須符合正態分佈,以上都不是,D
72,下面不是迭代二叉樹3代算法對數據的要求,所有的訓練例的所有屬性必須有一個明確的值,所有屬性必須爲離散量,所有屬性必須爲連續,相同的因素必須得到相同的結論且訓練例必須唯一,C
73,下面的優化算法中，速度最快的是 ,BFGS,梯度下降法,牛頓法,Adam,C
74,下列關於ALBERT的說法不正確的是 ,跨層參數共享,採用詞嵌入向量參數的因式分解,應用在下游任務中預測速度顯著提速,去掉了dropout,D
75,下面哪個屬於SVM應用,新文章聚類,文本和超文本分類,圖像分類,以上均是,D
76,類域界面方程法中，不能求線性不可分情況下分類問題近似或精確解的方法是,基於二次準則的H-K算法,感知器算法 ,勢函數法,僞逆法,B
77,下面哪個選項中哪一項屬於確定性算法,K-Means,PCA,KNN,以上都不是,B
78,"以下哪些算法, 1. KNN；2. 線性迴歸；3.對數幾率迴歸。可以用神經網絡去構造:",2 和 3,1和 2,以上都不是,"1, 2 和 3",A
79,訓練SVM的最小時間複雜度爲O(n2)，那麼一下哪種數據集不適合用SVM?,和數據集大小無關,大數據集,小數據集,中等大小數據集,B
80,如果線性迴歸模型中的隨機誤差存在異方差性，那麼參數的普通最小二乘法估計量是,無偏的，非有效的,無偏的，有效的,有偏的，非有效的,有偏的，有效的,A
81,下列關於RoBERTa的說法不正確的是 ,不做NSP任務,採用靜態掩碼機制,採用更多訓練數據,訓練採用更大batch size,B
82,在邏輯迴歸輸出與目標對比的情況下，以下評估指標中哪一項不適用？,準確度,均方誤差,AUC-ROC,Logloss,B
83,語言模型的參數估計經常使用MLE（最大似然估計）。面臨的一個問題是沒有出現的項概率爲0，這樣會導致語言模型的效果不好。爲了解決這個問題，需要使用（）,增加白噪音,平滑,隨機插值,去噪,B
84,建模北京市人口的年齡分佈，採用什麼分佈更合適,0-1分佈,正態分佈,泊松分佈,指數分佈,B
85,SVM中的代價參數表示：,誤分類與模型複雜性之間的平衡,以上均不是,使用的核,交叉驗證的次數,A
86,關於SVM泛化誤差描述正確的是,超平面與支持向量之間距離,SVM的誤差閾值,以上都不是,SVM對未知數據的預測能力,D
87,下列關於BERT的說法不正確的是 ,支持對語義上下文進行建模,採用激活函數GELU,網絡一共有20層,使用transformer,C
88,模式識別中，不屬於馬式距離較之於歐式距離的優點的是,尺度不變性,平移不變性,考慮到各種特性之間的聯繫,考慮了模式的分佈,B
89,描述的機器發生故障的次數，採用什麼分佈更合適? ,0-1分佈,指數分佈,正態分佈,泊松分佈,D
90,以下哪個不是LSTM本身的特點 ,LSTM是RNN的一種變種,防止梯度彌散,訓練時GPU使用率較高,LSTM有遺忘門,C
91,關於邏輯迴歸和支持向量機不正確的是,邏輯迴歸本質上是一種根據樣本對權值進行極大似然估計的方法，而後驗概率正比於先驗概率和似然函數的乘積。邏輯僅僅是最大化似然函數，並沒有最大化後驗概率，更談不上最小化後驗概率,支持向量機可以通過正則化係數控制模型的複雜度，避免過擬合。,支持向量機的目標是找到使得訓練數據儘可能分開且分類間隔最大的超平面，應該屬於結構風險最小化,邏輯迴歸的輸出就是樣本屬於正類別的幾率，可以計算出概率。,A
92,以下不屬於影響聚類算法結果的主要因素有,特徵選取,已知類別的樣本質量,分類準則,模式相似性測度,B
93,高斯混合模型(GMM)採用什麼準則進行訓練,均方誤差最小化,經驗風險最小化,期望最大化,其他選項都不是,C
94,以下說法正確的是,一個機器學習模型，如果有較高準確率，並不難總是說明這個分類器是好的,我們不可以使用聚類“類別id”作爲一個新的特徵項， 然後再用監督學習分別進行學習,如果增加模型複雜度， 那麼模型的測試錯誤率總是會降低,如果增加模型複雜度， 那麼模型的訓練錯誤率總是會降低,A
95,中文同義詞替換時，常用到Word2Vec，以下說法錯誤的是,Word2Vec結果符合當前預料環境,Word2Vec得到的都是語義上的同義詞,Word2Vec受限於訓練語料的數量和質量,Word2Vec基於概率統計,B
96,"在以下不同的場景中,使用的分析方法不正確的有","根據商家最近一年的經營及服務數據,用聚類算法判斷出天貓商家在各自主營類目下所屬的商家層級","根據商家近幾年的成交數據,用聚類算法擬合出用戶未來一個月可能的消費金額公式","用關聯規則算法分析出購買了汽車坐墊的買家,是否適合推薦汽車腳墊","根據用戶最近購買的商品信息,用決策樹算法識別出淘寶買家可能是男還是女",B
97,在數據清理中，下面哪個不是處理缺失值的方法,變量刪除,估算,整例刪除,成對刪除,D
98,以下關於LDA(Latent Dirichlet allocation)的說法錯誤的是,LDA是非監督學習技術,LDA可通過EM的思想求解,當選取一篇文檔後，對於該文檔主題的分佈是確定的,LDA包含詞，主題和文檔三層結構,C
99,如果我使用數據集的全部特徵並且能夠達到100%的準確率，但在測試集上僅能達到70%左右，這說明：,以上均不正確,欠擬合,過擬合,模型很棒,C
100,假定你使用SVM學習數據X，數據X裏面有些點存在錯誤。現在如果你使用一個二次核函數，多項式階數爲2，使用鬆弛變量C作爲超參之一。 當你使用較大的C（C趨於無窮），則：,以上均不正確,不確定,不能正確分類,仍然能正確分類數據,D
101,基於統計的分詞方法爲,正向量最大匹配法,條件隨機場,最少切分,逆向量最大匹配法,B
102,假定某同學使用假定某同學使用樸素貝葉斯分類模型時，不小心將訓練數據的兩個維度搞重複了，那麼關於樸素貝葉斯的說法中不正確的是,模型效果相比無重複特徵的情況下精確度會降低,模型效果相比無重複特徵的情況下精確度會提高,當兩列特徵高度相關時，無法用兩列特徵相同時所得到的結論來分析問題,如果所有特徵都被重複一遍，得到的模型預測結果相對於不重複的情況下的模型預測結果一樣,D
103,下列關於word2vec的說法中錯誤的是,使用詞向量可得到以下等式:King - man + woman = Queen,Skip-gram是給定詞窗中的文本，預測當前詞的概率,word2vec的假設是詞袋模型，詞的順序是不重要的,word2vec訓練中使用了Negative Sample與Hierarchical Softmax兩種加速算法,B
104,以下描述正確的是 ,聚類分析可以看作是一種非監督的分類。,在聚類分析當中，簇內的相似性越大，簇間的差別越大，聚類的效果就越差。,SVM是這樣一個分類器，他尋找具有最小邊緣的超平面，因此它也經常被稱爲最小邊緣分類器,在決策樹中，隨着樹中結點數變得太大，即使模型的訓練誤差還在繼續減低，但是檢驗誤差開始增大，這是出現了模型擬合不足的問題。,A
105,"下列選項中,識別模式與其他不⼀樣的是",出⾏方式判斷:步⾏、騎車、坐車,⽤戶年齡分佈判斷:少年、青年、中年、⽼年,投遞員分揀信件,醫⽣給病⼈診斷髮病類型,A
106,下列說法不正確的是,梯度下降法是利用當前位置的負梯度作爲搜索方向的方法,共軛梯度法僅需利用一階導數的信息，但是收斂速度高於梯度下降法,批量梯度下降和隨機梯度下降相比，批量梯度下降優勢是對於大規模樣本效率很高,牛頓法和梯度下降法相比，一個劣勢是求解複雜，一個優勢是收斂速度加快,C
107,"在隱馬爾科夫模型中,如果已知觀察序列和產生觀察序列的狀態序列,那麼可用以下哪種方法直接進行參數估計",前向後向算法,極大似然估計 ,維特比算法,EM算法,B
108,LDA(Latent Dirichlet allocation)中歸屬於同一主題下單詞分佈的先驗分佈是? ,正態分佈,狄利克雷分佈,多項分佈,二項分佈,C
109,線性迴歸的基本假設不包括哪個,對於解釋變量的所有觀測值，隨機誤差項有相同的方差,隨機誤差項是一個期望值爲0的隨機變量,隨機誤差項服從正態分佈,隨機誤差項彼此相關,D
110,下列不是SVM核函數的是,Sigmoid核函數,徑向基核函數,多項式核函數,logistic核函數,D
111,下列哪些是非監督學習方法 ,SVM,K-means,KNN,決策樹,B
112,以下哪種方法屬於判別式模型,貝葉斯網絡,樸素貝葉斯,隱馬模型 ,支持向量機,D
113,下面的交叉驗證方法：i. 有放回的Bootstrap方法；ii. 留一個測試樣本的交叉驗證；iii. 5折交叉驗證；iv. 重複兩次的5折教程驗證。當樣本是1000時，下面執行時間的順序，正確的是,ii > iv > iii > i,ii > iii > iv > i,iv > i > ii > iii,i > ii > iii > iv,A
114,Seq2Seq模型在解碼時可以選用的方法 ,貪心算法,二者均可,Beam Search,二者均不可,B
115,解決隱馬模型中預測問題的算法是,前向算法,維特比算法,Baum-Welch算法,後向算法,B
116,以下對k-means聚類算法解釋正確的是,"能自動識別類的個數,不是隨即挑選初始點爲中心點計算","不能自動識別類的個數,不是隨即挑選初始點爲中心點計算","不能自動識別類的個數,隨即挑選初始點爲中心點計算","能自動識別類的個數,隨即挑選初始點爲中心點計算",C
117,一般，k-NN最近鄰方法在（）的情況下效果較好,樣本呈團狀分佈,樣本較多但典型性不好,樣本呈鏈狀分佈,樣本較少但典型性好,D
118,一監獄人臉識別准入系統用來識別待進入人員的身份，此係統一共包括識別4種不同的人員：獄警，小偷，送餐員，其他。下面哪種學習方法最適合此種應用需求,多分類問題,二分類問題,k-中心點聚類問題,層次聚類問題,A
119,爲了得到和 SVD 一樣的投射（projection），你需要在 PCA 中怎樣做,將數據轉換成零均值,無法做到,將數據轉換成零衆數,將數據轉換成零中位數,A
120,在統計模式分類問題中，當先驗概率未知時，可以使用,N-P判決,最小最大損失準則,最小損失準則,最小誤判概率準則,B
121,以下哪些方法不可以直接來對文本分類,決策樹,Kmeans,支持向量機,KNN,B
